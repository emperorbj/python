import os
from langchain_community.vectorstores import FAISS
from langchain_cohere import CohereEmbeddings
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate # Import prompt template
from langchain.chains.combine_documents import create_stuff_documents_chain # For combining docs
from langchain.chains import create_retrieval_chain # For the full RAG chain

load_dotenv()


current_dir = os.path.dirname(os.path.abspath(__file__))
persistent_dir = os.path.join(current_dir,"faiss_index")


embeddings = CohereEmbeddings(
    model="embed-v4.0",
    cohere_api_key=os.getenv("COHERE_API_KEY")
)

if not os.path.exists(persistent_dir):
    print(f"This path {persistent_dir} does not exist")
    
print("Initializing this process")

vector_store = FAISS.load_local(persistent_dir,embeddings,allow_dangerous_deserialization=True)
print("Faiss index store loaded")

llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.2)
print("Google Generative AI LLM initialized.")





# --- Set up the RAG Chain ---

# 1. Create a Retriever from the FAISS vector store
# This component will search your vector store for relevant documents based on the query.
retriever = vector_store.as_retriever(search_kwargs={"k": 3}) # Retrieve top 3 documents

# 2. Define the Prompt Template for the LLM
# This template tells the LLM how to use the retrieved context to answer the question.
prompt = ChatPromptTemplate.from_template("""
You are an AI assistant tasked with answering questions based on the provided context.
Carefully read the context and use only the information within it to answer the question.
If the answer cannot be found in the context, state that clearly and do not make up information.

<context>
{context}
</context>

Question: {input}
""")

# 3. Create a Document Combining Chain
# This chain takes the retrieved documents and "stuffs" them into the prompt template.
document_chain = create_stuff_documents_chain(llm, prompt)

# 4. Create the full Retrieval Augmented Generation (RAG) Chain
# This combines the retriever (to get context) and the document_chain (to generate the answer).
retrieval_rag_chain = create_retrieval_chain(retriever, document_chain)
print("RAG chain setup complete.")

# --- Interact with the User for RAG ---
print("\n--- Ask me anything based on the documents! ---")
while True:
    user_query = input("Please add your query (or type 'exit' to leave): ").strip()

    if user_query.lower() == "exit":
        print("Thanks, exiting now!")
        break

    if not user_query:
        print("Please provide an input. The query cannot be empty.")
        continue

    print(f"\nProcessing your query: '{user_query}'...")

    # Invoke the RAG chain
    # This will first retrieve documents, then pass them to the LLM with the query
    response = retrieval_rag_chain.invoke({"input": user_query})

    # The 'response' object will contain the 'answer' generated by the LLM
    # and optionally 'context' if you want to see the retrieved documents
    print("\n--- Generated Answer ---")
    print(response["answer"])

    # Optional: Display the retrieved context for debugging/verification
    # print("\n--- Retrieved Context (for reference) ---")
    # for i, doc in enumerate(response["context"]):
    #     print(f"Document {i+1}:\n{doc.page_content}")
    #     if doc.metadata:
    #         print(f"Metadata: {doc.metadata}")
    #     print("-" * 100)
    print("-" * 120)







# while True:
#     user_query = input("Please add your query(or type exit to leave):").strip()
#     if user_query.lower() == "exit":
#         print(f"Thanks leaving now")
#         break
#     if not user_query:
#         print("Please you have to add an input")
#         continue
    
#     print(f"The query you want to make is: {user_query}")
#     retrieved_docs = vector_store.similarity_search(user_query,k=3)
    
#     if retrieved_docs:
#         print(f"\n Found {len(retrieved_docs)} in chunks from the documents on the user query")
#         for i,docs in enumerate(retrieved_docs):
#             print(f"{i + 1} {docs.page_content}")
#             if docs.metadata:
#                 print(f"This the metadata for {docs.metadata}")
                
#             print("-"*120)
#     else:
#         print("No relevant information found in this document")